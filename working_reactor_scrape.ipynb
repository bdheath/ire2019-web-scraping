{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import the libraries we'll need\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Use the requests library to get the HTML for the NRC list of reactor units\n",
    "url = 'https://www.nrc.gov/reactors/operating/list-power-reactor-units.html'\n",
    "r = requests.get(url)\n",
    "\n",
    "\n",
    "# Check the status code. \"200\" means it worked\n",
    "r.status_code\n",
    "\n",
    "\n",
    "# Parse the HTML into a beautiful soup\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the HTML table within the soup\n",
    "table = soup.find('table')\n",
    "\n",
    "# Find the rows within the table\n",
    "rows = table.find_all('tr')[1:]\n",
    "\n",
    "# Create an empty list to hold the values we scrape from the table\n",
    "data = []\n",
    "\n",
    "# Go row by row to extract the data we want\n",
    "for row in rows:\n",
    "    # Find each cell in the row\n",
    "    cells = row.find_all('td')\n",
    "    \n",
    "    # Find values in the cells in the row, and assign them to variables \n",
    "    plant = cells[0].find('a').string\n",
    "    license = cells[1].string\n",
    "    owner = cells[4].string\n",
    "        \n",
    "    print(plant)\n",
    "    \n",
    "    # Now something fancy: Build a URL from the link in the first column\n",
    "    link = 'http://www.nrc.gov' + cells[0].find('a')['href']\n",
    "    # And fetch that URL in case we want additional data from that page\n",
    "    # (Note that we're using different variable names)\n",
    "    sr = requests.get(link)\n",
    "    \n",
    "    # Add the scraped values to the list we set up earlier\n",
    "    data.append([plant, license, owner])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Now we have all the values in the 'data' list to a CSV\n",
    "\n",
    "headers = ['Plant Name','License Number','Owner']\n",
    "with open('nrc.csv','w',newline='') as outfile:\n",
    "    writer = csv.writer(outfile, delimiter='\\t')\n",
    "    writer.writerow(headers)\n",
    "    writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions? bheath@usatoday.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
